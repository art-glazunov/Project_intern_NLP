{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0WFK0qwKtz6z"
   },
   "source": [
    "# <center> Typical text preprocessing  \n",
    "# <center> and architectures comparison example\n",
    "\n",
    "### <center> Glazunov A.V."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5S5OcseJyOaI"
   },
   "source": [
    "Works via Google Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 904
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 24633,
     "status": "ok",
     "timestamp": 1594537033788,
     "user": {
      "displayName": "Artem Glazunov",
      "photoUrl": "",
      "userId": "00284840930055510851"
     },
     "user_tz": -180
    },
    "id": "usIOsG0PHn1P",
    "outputId": "efe57389-b6d0-4122-d9f1-f229d296019c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting num2words\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/eb/a2/ea800689730732e27711c41beed4b2a129b34974435bdc450377ec407738/num2words-0.5.10-py3-none-any.whl (101kB)\n",
      "\r",
      "\u001b[K     |███▎                            | 10kB 17.3MB/s eta 0:00:01\r",
      "\u001b[K     |██████▌                         | 20kB 2.0MB/s eta 0:00:01\r",
      "\u001b[K     |█████████▊                      | 30kB 2.7MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████                   | 40kB 3.1MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████▏               | 51kB 2.4MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████▍            | 61kB 2.7MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████▋         | 71kB 2.9MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████▉      | 81kB 3.1MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████   | 92kB 3.3MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 102kB 2.5MB/s \n",
      "\u001b[?25hRequirement already satisfied: docopt>=0.6.2 in /usr/local/lib/python3.6/dist-packages (from num2words) (0.6.2)\n",
      "Installing collected packages: num2words\n",
      "Successfully installed num2words-0.5.10\n",
      "Collecting pymorphy2\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/33/fff9675c68b5f6c63ec8c6e6ff57827dda28a1fa5b2c2d727dffff92dd47/pymorphy2-0.8-py2.py3-none-any.whl (46kB)\n",
      "\u001b[K     |████████████████████████████████| 51kB 1.5MB/s \n",
      "\u001b[?25hCollecting pymorphy2-dicts<3.0,>=2.4\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/02/51/2465fd4f72328ab50877b54777764d928da8cb15b74e2680fc1bd8cb3173/pymorphy2_dicts-2.4.393442.3710985-py2.py3-none-any.whl (7.1MB)\n",
      "\u001b[K     |████████████████████████████████| 7.1MB 7.3MB/s \n",
      "\u001b[?25hRequirement already satisfied: docopt>=0.6 in /usr/local/lib/python3.6/dist-packages (from pymorphy2) (0.6.2)\n",
      "Collecting dawg-python>=0.7\n",
      "  Downloading https://files.pythonhosted.org/packages/6a/84/ff1ce2071d4c650ec85745766c0047ccc3b5036f1d03559fd46bb38b5eeb/DAWG_Python-0.7.2-py2.py3-none-any.whl\n",
      "Installing collected packages: pymorphy2-dicts, dawg-python, pymorphy2\n",
      "Successfully installed dawg-python-0.7.2 pymorphy2-0.8 pymorphy2-dicts-2.4.393442.3710985\n",
      "Collecting natasha\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/26/d2/2b1d94e4d26e6f6098b3c9746253de797c1c6f9cfb883c45761e86382b2a/natasha-1.2.0-py3-none-any.whl (34.4MB)\n",
      "\u001b[K     |████████████████████████████████| 34.4MB 7.1MB/s \n",
      "\u001b[?25hCollecting razdel>=0.5.0\n",
      "  Downloading https://files.pythonhosted.org/packages/15/2c/664223a3924aa6e70479f7d37220b3a658765b9cfe760b4af7ffdc50d38f/razdel-0.5.0-py3-none-any.whl\n",
      "Collecting slovnet>=0.3.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c2/6f/1c989335c9969421f771e4f0410ba70d82fe992ec9f3cbac9f432d8f5733/slovnet-0.4.0-py3-none-any.whl (49kB)\n",
      "\u001b[K     |████████████████████████████████| 51kB 4.1MB/s \n",
      "\u001b[?25hCollecting ipymarkup>=0.8.0\n",
      "  Downloading https://files.pythonhosted.org/packages/bf/9b/bf54c98d50735a4a7c84c71e92c5361730c878ebfe903d2c2d196ef66055/ipymarkup-0.9.0-py3-none-any.whl\n",
      "Requirement already satisfied: pymorphy2 in /usr/local/lib/python3.6/dist-packages (from natasha) (0.8)\n",
      "Collecting yargy>=0.14.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8e/07/94306844e3a5cb520660612ad98bce56c168edb596679bd541e68dfde089/yargy-0.14.0-py3-none-any.whl (41kB)\n",
      "\u001b[K     |████████████████████████████████| 51kB 4.6MB/s \n",
      "\u001b[?25hCollecting navec>=0.9.0\n",
      "  Downloading https://files.pythonhosted.org/packages/83/ad/554945ebee66fe83fefd61e043938981dd9e6136882025c506ac6faa6a4c/navec-0.9.0-py3-none-any.whl\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from slovnet>=0.3.0->natasha) (1.18.5)\n",
      "Collecting intervaltree>=3\n",
      "  Downloading https://files.pythonhosted.org/packages/e8/f9/76237755b2020cd74549e98667210b2dd54d3fb17c6f4a62631e61d31225/intervaltree-3.0.2.tar.gz\n",
      "Requirement already satisfied: dawg-python>=0.7 in /usr/local/lib/python3.6/dist-packages (from pymorphy2->natasha) (0.7.2)\n",
      "Requirement already satisfied: pymorphy2-dicts<3.0,>=2.4 in /usr/local/lib/python3.6/dist-packages (from pymorphy2->natasha) (2.4.393442.3710985)\n",
      "Requirement already satisfied: docopt>=0.6 in /usr/local/lib/python3.6/dist-packages (from pymorphy2->natasha) (0.6.2)\n",
      "Requirement already satisfied: sortedcontainers<3.0,>=2.0 in /usr/local/lib/python3.6/dist-packages (from intervaltree>=3->ipymarkup>=0.8.0->natasha) (2.2.2)\n",
      "Building wheels for collected packages: intervaltree\n",
      "  Building wheel for intervaltree (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for intervaltree: filename=intervaltree-3.0.2-cp36-none-any.whl size=25791 sha256=4ef5065fe2db19faf15c50c5316943a4ef1e220f40730001af5b3f1c78c51e2e\n",
      "  Stored in directory: /root/.cache/pip/wheels/08/99/c0/5a5942f5b9567c59c14aac76f95a70bf11dccc71240b91ebf5\n",
      "Successfully built intervaltree\n",
      "Installing collected packages: razdel, navec, slovnet, intervaltree, ipymarkup, yargy, natasha\n",
      "  Found existing installation: intervaltree 2.1.0\n",
      "    Uninstalling intervaltree-2.1.0:\n",
      "      Successfully uninstalled intervaltree-2.1.0\n",
      "Successfully installed intervaltree-3.0.2 ipymarkup-0.9.0 natasha-1.2.0 navec-0.9.0 razdel-0.5.0 slovnet-0.4.0 yargy-0.14.0\n"
     ]
    }
   ],
   "source": [
    "!pip install num2words\n",
    "!pip install pymorphy2\n",
    "!pip install natasha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 46850,
     "status": "ok",
     "timestamp": 1594537056951,
     "user": {
      "displayName": "Artem Glazunov",
      "photoUrl": "",
      "userId": "00284840930055510851"
     },
     "user_tz": -180
    },
    "id": "MrStqZG0IDJv",
    "outputId": "f59ffc07-dd2f-44c5-caa1-44eb4ace397b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK Downloader\n",
      "---------------------------------------------------------------------------\n",
      "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
      "---------------------------------------------------------------------------\n",
      "Downloader> d\n",
      "\n",
      "Download which package (l=list; x=cancel)?\n",
      "  Identifier> stopwords\n",
      "    Downloading package stopwords to /root/nltk_data...\n",
      "      Unzipping corpora/stopwords.zip.\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
      "---------------------------------------------------------------------------\n",
      "Downloader> q\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 694,
     "status": "ok",
     "timestamp": 1594537060115,
     "user": {
      "displayName": "Artem Glazunov",
      "photoUrl": "",
      "userId": "00284840930055510851"
     },
     "user_tz": -180
    },
    "id": "0VNmd4ANHP-t"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import pymorphy2\n",
    "from num2words import num2words\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "from natasha import (\n",
    "    Segmenter,\n",
    "    MorphVocab,\n",
    "    \n",
    "    NewsEmbedding,\n",
    "    NewsMorphTagger,\n",
    "    NewsNERTagger,\n",
    "\n",
    "    NamesExtractor,\n",
    "    AddrExtractor,\n",
    "    Doc\n",
    ")\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 594,
     "status": "ok",
     "timestamp": 1594537640895,
     "user": {
      "displayName": "Artem Glazunov",
      "photoUrl": "",
      "userId": "00284840930055510851"
     },
     "user_tz": -180
    },
    "id": "TtqXR5ZAHZ9D"
   },
   "outputs": [],
   "source": [
    "#My little collection of functions for preprocessing for a single text\n",
    "\n",
    "\n",
    "def emoji_replacer(text,emoji_list,replacers):\n",
    "  #Transform emoji into words\n",
    "\n",
    "  for index,emoji in enumerate(emoji_list):\n",
    "    text = text.replace(emoji,' '+ replacers[index] +' ')\n",
    "\n",
    "  return text\n",
    "\n",
    "\n",
    "def text_early_preproc(text,del_html = True,del_punct_sp_chars=True,\n",
    "                 del_underscore=True, del_digits=False):\n",
    "  #Clean the text from artifacts and punctuation\n",
    "\n",
    "  #Delete whitespaces and special string symbols\n",
    "  text = re.sub(\"^\\s+|\\n|\\r|\\s+$\", ' ', text)\n",
    "\n",
    "  #Delete html tags\n",
    "  if del_html:\n",
    "    soap = BeautifulSoup(text, 'html.parser')\n",
    "    text = soap.get_text()\n",
    "\n",
    "  #Delete punctuation and other artifacts\n",
    "  if del_punct_sp_chars:\n",
    "    text = re.sub(r'[^\\w\\s]','',text)\n",
    "\n",
    "  #Delete '_'\n",
    "  if del_underscore:\n",
    "    text = text.replace('_','')\n",
    "\n",
    "  #Delete digits\n",
    "  if del_digits:\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "\n",
    "                 \n",
    "  return text\n",
    "\n",
    "\n",
    "def lemmatize_lower_case(text):\n",
    "  #lemmatizationa in lower case\n",
    "\n",
    "  words = text.lower().split()\n",
    "  morph = pymorphy2.MorphAnalyzer()\n",
    "\n",
    "  normal_tokens= [morph.parse(word)[0].normal_form for word in words]\n",
    "\n",
    "  return \" \".join(normal_tokens)\n",
    "\n",
    "\n",
    "def delete_stop_words(text):\n",
    "  #delete Russian stopwords\n",
    "\n",
    "  tokens = [token for token in text.split() if token not in stopwords.words(\"russian\")]\n",
    "  text = \" \".join(tokens)\n",
    "  return text\n",
    "\n",
    "def numbers_to_text(text):\n",
    "  #Converts numbers into Russian text\n",
    "\n",
    "  tokens = text.split()\n",
    "  text = \" \".join([num2words(token,lang='ru') if token.isnumeric() else token for token in tokens ])\n",
    "  \n",
    "  return text\n",
    "\n",
    "\n",
    "def delete_digits(text):\n",
    "  #Delete digits\n",
    "  text = re.sub(r'\\d+', '', text)\n",
    "\n",
    "  return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 693,
     "status": "ok",
     "timestamp": 1594537642567,
     "user": {
      "displayName": "Artem Glazunov",
      "photoUrl": "",
      "userId": "00284840930055510851"
     },
     "user_tz": -180
    },
    "id": "USxQz4Deim0m"
   },
   "outputs": [],
   "source": [
    "#Functions to work with named entities\n",
    "\n",
    "def ne_extraction(text,\n",
    "                  segmenter,morph_vocab,\n",
    "                  morph_tagger,ner_tagger,\n",
    "                  del_names=False,del_addr=False):\n",
    "\n",
    "\n",
    "  #Extract, normalize and delete (optional) named entities\n",
    "\n",
    "\n",
    "  doc = Doc(text)\n",
    "\n",
    "  doc.segment(segmenter)\n",
    "  doc.tag_ner(ner_tagger)\n",
    "  doc.tag_morph(morph_tagger)\n",
    "\n",
    "\n",
    "  for span in doc.spans:\n",
    "    span.normalize(morph_vocab)\n",
    "\n",
    "  for span in doc.spans:\n",
    "\n",
    "    if span.type == 'PER':\n",
    "      span.extract_fact(names_extractor)\n",
    "\n",
    "    if span.type == 'LOC':\n",
    "      span.extract_fact(addr_extractor)\n",
    "\n",
    "  if del_names:\n",
    "    for span in doc.spans:\n",
    "      if span.type == 'PER':\n",
    "        text = text.replace(span.text,'')\n",
    "\n",
    "\n",
    "  if del_addr:\n",
    "    for span in doc.spans:\n",
    "      if span.type == 'LOC':\n",
    "        text = text.replace(span.text,'')\n",
    "\n",
    "  normal_ne = {}\n",
    "  normal_ne['NAMES'] = list(np.unique([span.normal for span in doc.spans if span.type == 'PER']))\n",
    "  normal_ne['LOCATIONS'] = list(np.unique([span.normal for span in doc.spans if span.type == 'LOC']))\n",
    "\n",
    "  return text, normal_ne\n",
    "\n",
    "\n",
    "def add_normal_ne(text,normal_ne):\n",
    "  #Add extracted and normalized named entities in the end\n",
    "\n",
    "  names = [\"_\".join(ne.split()) for ne in normal_ne['NAMES']]\n",
    "\n",
    "  locations = [\"_\".join(ne.split()) for ne in normal_ne['LOCATIONS']]\n",
    "\n",
    "  return \" \".join([text] + names + locations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 619,
     "status": "ok",
     "timestamp": 1594537940584,
     "user": {
      "displayName": "Artem Glazunov",
      "photoUrl": "",
      "userId": "00284840930055510851"
     },
     "user_tz": -180
    },
    "id": "dJ7sI_rLIBJB"
   },
   "outputs": [],
   "source": [
    "text = '''<div> \n",
    "\n",
    "<p> \n",
    "\n",
    "Очень хочу поздравить своего хорошего друга и учителя Александра Петровича Иванова, сегодня ему\n",
    "\n",
    "50 лет11!!!:)\n",
    "Великолепная дата, поэтому желаю ему, чтобы был здоров    и весел, и прожил еще 100500 лет!1!11!!|\\\\\\\\\n",
    "\n",
    "Александру Петровичу Иванову респект!!\n",
    "\n",
    "Короче, все это прекрасно, и теперь приступим к застолью, господа!!!\n",
    "\n",
    "Я щас!!:))))\n",
    "\n",
    "Жаль, что завтра на работу:((((\n",
    "\n",
    "Конечно, люблю Саратов, но в Адлере сейчас лучше!!\n",
    "\n",
    "Но в любом случае передаю привет моей Любови Ильиничне Кизляркиной из Магадана!!\n",
    "\n",
    "\n",
    "\n",
    "</p> </div>\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 465,
     "status": "ok",
     "timestamp": 1594537941324,
     "user": {
      "displayName": "Artem Glazunov",
      "photoUrl": "",
      "userId": "00284840930055510851"
     },
     "user_tz": -180
    },
    "id": "_GrdbzQ9JbYW",
    "outputId": "e4929c80-1749-45b0-d279-da9d1f12017e"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic": {
       "type": "string"
      },
      "text/plain": [
       "'<div> \\n\\n<p> \\n\\nОчень хочу поздравить своего хорошего друга и учителя Александра Петровича Иванова, сегодня ему\\n\\n50 лет11!!!:)\\nВеликолепная дата, поэтому желаю ему, чтобы был здоров    и весел, и прожил еще 100500 лет!1!11!!|\\\\\\\\\\n\\nАлександру Петровичу Иванову респект!!\\n\\nКороче, все это прекрасно, и теперь приступим к застолью, господа!!!\\n\\nЯ щас!!:))))\\n\\nЖаль, что завтра на работу:((((\\n\\nКонечно, люблю Саратов, но в Адлере сейчас лучше!!\\n\\nНо в любом случае передаю привет моей Любови Ильиничне Кизляркиной из Магадана!!\\n\\n\\n\\n</p> </div>\\n\\n'"
      ]
     },
     "execution_count": 51,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1644,
     "status": "ok",
     "timestamp": 1594537776624,
     "user": {
      "displayName": "Artem Glazunov",
      "photoUrl": "",
      "userId": "00284840930055510851"
     },
     "user_tz": -180
    },
    "id": "yilB6Vpkr8bm"
   },
   "outputs": [],
   "source": [
    "#Initialise Natasha main tools\n",
    "\n",
    "segmenter = Segmenter()\n",
    "morph_vocab = MorphVocab()\n",
    "emb = NewsEmbedding()\n",
    "morph_tagger = NewsMorphTagger(emb)\n",
    "ner_tagger = NewsNERTagger(emb)\n",
    "\n",
    "names_extractor = NamesExtractor(morph_vocab)\n",
    "addr_extractor = AddrExtractor(morph_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 750,
     "status": "ok",
     "timestamp": 1594538274502,
     "user": {
      "displayName": "Artem Glazunov",
      "photoUrl": "",
      "userId": "00284840930055510851"
     },
     "user_tz": -180
    },
    "id": "1j9JOeMcf239",
    "outputId": "d4bcd4cb-36ee-4da0-85df-21c942e6ef3d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 172 ms, sys: 115 ms, total: 287 ms\n",
      "Wall time: 170 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#Example\n",
    "\n",
    "text_res,ne = ne_extraction(text,\n",
    "                  segmenter,morph_vocab,\n",
    "                  morph_tagger,ner_tagger,\n",
    "                  del_names=True,del_addr=True)\n",
    "\n",
    "emoji_list = [':)',':(']\n",
    "replacers = ['happy','sad']\n",
    "text_res = emoji_replacer(text_res,emoji_list,replacers)\n",
    "\n",
    "text_res = text_early_preproc(text_res,del_html = True,del_punct_sp_chars=True,\n",
    "                 del_underscore=True, del_digits=False)\n",
    "\n",
    "text_res = numbers_to_text(text_res)\n",
    "\n",
    "text_res = delete_digits(text_res)\n",
    "\n",
    "text_res = lemmatize_lower_case(text_res)\n",
    "\n",
    "text_res = delete_stop_words(text_res)\n",
    "\n",
    "text_res = add_normal_ne(text_res,ne)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 667,
     "status": "ok",
     "timestamp": 1594537959025,
     "user": {
      "displayName": "Artem Glazunov",
      "photoUrl": "",
      "userId": "00284840930055510851"
     },
     "user_tz": -180
    },
    "id": "CUV8e5ja3jpq",
    "outputId": "9dd10e22-6bb2-4a35-b01b-c4441311be20"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic": {
       "type": "string"
      },
      "text/plain": [
       "'очень хотеть поздравить свой хороший друг учитель сегодня пятьдесят год happy великолепный дата поэтому желать здоровый весёлый прожить ещё сто тысяча пятьсот год респект короче весь это прекрасно приступить застолье господин щас happy жаль завтра работа sad любить хороший любой случай передавать привет Александр_Петрович_Иванов Любовь_Ильинична_Кизляркина Адлер Магадан Саратов'"
      ]
     },
     "execution_count": 53,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Png1oWk3uGgn"
   },
   "source": [
    "## For the purpose of preprocessing a dataframe with lots of texts, after exploring several examples, I use all selected and tuned functions in a loop for all the texts.\n",
    "\n",
    "Altought the approach with many different fuctions would be convenient (we can easily debug and add/delete functionalities in our preprocessing loop), it wouldn't be the fastest algorithm, would it? In all this fuctions there are some duplicating operations (for example, split(), join()), and maybe many transfer operations aren't a good idea either. With this example, I want to roughly demonstrate the microservices and monoliths architectures comparison for a little prototype of an app. Now I will try to create some king of monolith function without described problems (but with other)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 647,
     "status": "ok",
     "timestamp": 1594538697770,
     "user": {
      "displayName": "Artem Glazunov",
      "photoUrl": "",
      "userId": "00284840930055510851"
     },
     "user_tz": -180
    },
    "id": "hSREv_zUzXH7"
   },
   "outputs": [],
   "source": [
    "def text_preprocessing(text, segmenter,morph_vocab,\n",
    "                      morph_tagger,ner_tagger,\n",
    "                      emoji_list, replacers,                   \n",
    "                      del_names=False,del_addr=False,\n",
    "                      del_html = True,del_punct_sp_chars=True,\n",
    "                      del_underscore=True):\n",
    "  \n",
    "  #Text preprocessing function\n",
    "\n",
    "\n",
    "  #Extract, normalize and delete (optional) named entities\n",
    "\n",
    "  doc = Doc(text)\n",
    "\n",
    "  doc.segment(segmenter)\n",
    "  doc.tag_ner(ner_tagger)\n",
    "  doc.tag_morph(morph_tagger)\n",
    "\n",
    "\n",
    "  for span in doc.spans:\n",
    "    span.normalize(morph_vocab)\n",
    "\n",
    "  for span in doc.spans:\n",
    "\n",
    "    if span.type == 'PER':\n",
    "      span.extract_fact(names_extractor)\n",
    "\n",
    "    if span.type == 'LOC':\n",
    "      span.extract_fact(addr_extractor)\n",
    "\n",
    "  if del_names:\n",
    "    for span in doc.spans:\n",
    "      if span.type == 'PER':\n",
    "        text = text.replace(span.text,'')\n",
    "\n",
    "\n",
    "  if del_addr:\n",
    "    for span in doc.spans:\n",
    "      if span.type == 'LOC':\n",
    "        text = text.replace(span.text,'')\n",
    "\n",
    "  normal_ne = {}\n",
    "  normal_ne['NAMES'] = list(np.unique([span.normal for span in doc.spans if span.type == 'PER']))\n",
    "  normal_ne['LOCATIONS'] = list(np.unique([span.normal for span in doc.spans if span.type == 'LOC']))\n",
    "\n",
    "  #Transform emoji into words\n",
    "\n",
    "  for index,emoji in enumerate(emoji_list):\n",
    "    text = text.replace(emoji,' '+ replacers[index] +' ')\n",
    "\n",
    "\n",
    "  #Clean the text from artifacts and punctuation\n",
    "\n",
    "  #Delete whitespaces and special string symbols\n",
    "  text = re.sub(\"^\\s+|\\n|\\r|\\s+$\", ' ', text)\n",
    "\n",
    "  #Delete html tags\n",
    "  if del_html:\n",
    "    soap = BeautifulSoup(text, 'html.parser')\n",
    "    text = soap.get_text()\n",
    "\n",
    "  #Delete punctuation and other artifacts\n",
    "  if del_punct_sp_chars:\n",
    "    text = re.sub(r'[^\\w\\s]','',text)\n",
    "\n",
    "  #Delete '_'\n",
    "  if del_underscore:\n",
    "    text = text.replace('_','')\n",
    "                 \n",
    "\n",
    "  #Converts numbers into Russian text,\n",
    "  #lemmatize in lower case\n",
    "  #delete Russian stopwords and digits\n",
    "  # in a single loop  \n",
    "\n",
    "  morph = pymorphy2.MorphAnalyzer()\n",
    "\n",
    "  tokens = text.lower().split()\n",
    "\n",
    "  new_tokens = []\n",
    "  for token in tokens:\n",
    "    \n",
    "    if token.isnumeric():\n",
    "      token = \" \".join([morph.parse(word)[0].normal_form for word in num2words(token,lang='ru').split()])\n",
    "    else:\n",
    "      token = re.sub(r'\\d+', '', token)\n",
    "      token = morph.parse(token)[0].normal_form\n",
    "\n",
    "    if token not in stopwords.words(\"russian\"):\n",
    "      new_tokens.append(token)\n",
    "  \n",
    "  #Add extracted and normalized named entities in the end\n",
    "\n",
    "  names = [\"_\".join(ne.split()) for ne in normal_ne['NAMES']]\n",
    "\n",
    "  locations = [\"_\".join(ne.split()) for ne in normal_ne['LOCATIONS']]\n",
    "\n",
    "  return \" \".join(new_tokens + names + locations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 627,
     "status": "ok",
     "timestamp": 1594538706938,
     "user": {
      "displayName": "Artem Glazunov",
      "photoUrl": "",
      "userId": "00284840930055510851"
     },
     "user_tz": -180
    },
    "id": "dDxezFf4ztqO",
    "outputId": "2ebe0ddf-f6df-420b-efd1-4cb6a3ee1cec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 167 ms, sys: 113 ms, total: 280 ms\n",
      "Wall time: 153 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#Another example\n",
    "\n",
    "text_res = text_preprocessing(text, segmenter,morph_vocab,\n",
    "                      morph_tagger,ner_tagger,\n",
    "                      emoji_list, replacers,                   \n",
    "                      del_names=True,del_addr=True,\n",
    "                      del_html = True,del_punct_sp_chars=True,\n",
    "                      del_underscore=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 610,
     "status": "ok",
     "timestamp": 1594538304425,
     "user": {
      "displayName": "Artem Glazunov",
      "photoUrl": "",
      "userId": "00284840930055510851"
     },
     "user_tz": -180
    },
    "id": "HgAe0Dtd00lb",
    "outputId": "f7784ea2-3d72-4b64-a30a-1b834f24d172"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic": {
       "type": "string"
      },
      "text/plain": [
       "'очень хотеть поздравить свой хороший друг учитель сегодня пятьдесят год happy великолепный дата поэтому желать здоровый весёлый прожить ещё сто тысяча пятьсот год респект короче весь это прекрасно приступить застолье господин щас happy жаль завтра работа sad любить хороший любой случай передавать привет Александр_Петрович_Иванов Любовь_Ильинична_Кизляркина Адлер Магадан Саратов'"
      ]
     },
     "execution_count": 81,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wP56Phze47U8"
   },
   "source": [
    "We got a code, that is hard to maintain, but it's slightly faster (of course, there can be some random causes, and this time difference is statistically insignificant) and shorter, when it comes to type the code. \n",
    "\n",
    "Personally, I would use the first version with many functions, because it is easier to tune.\n",
    "\n",
    "## So, it was my little example of the monoliths and microservices comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 675,
     "status": "ok",
     "timestamp": 1594538299538,
     "user": {
      "displayName": "Artem Glazunov",
      "photoUrl": "",
      "userId": "00284840930055510851"
     },
     "user_tz": -180
    },
    "id": "sCgHm-NI3X94"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPJFOVUxlqPlJWCKHWXbljI",
   "collapsed_sections": [],
   "name": "Typical_preprocessing_and_architectures_comparison.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
